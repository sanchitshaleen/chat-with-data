# Backend Req: Common in all environments:
ipykernel==6.29.5
langchain==0.3.25
langchain-community==0.3.24
langchain-core==0.3.60
langchain-text-splitters==0.3.8
PyMuPDF==1.25.5
unstructured==0.17.2
Markdown==3.8
# faiss-cpu==1.11.0  # removed (migrated to Qdrant)
fastapi==0.115.12
uvicorn==0.34.2
python-multipart==0.0.20
qdrant-client==1.12.1
pytz==2025.2
bcrypt==4.3.0
psycopg2-binary==2.9.9
redis==5.2.1

# Retrieval Enhancements:
voyageai==0.3.3  # Voyage AI Rerank 2.5 for re-ranking
fastembed>=0.4.0  # FastEmbed with SparseTextEmbedding for SPLADE sparse vectors
sentence-transformers==3.0.1  # SPLADE-v3 sparse embeddings and other transformers
# DBSF and MMR are handled by Qdrant native Query API (v1.10.0+)

# Frontend Req: If you want to serve streamlit app:
pytz==2025.2
streamlit==1.45.1

# Backend Req: One of these two as per your convenience:
# I have used ollama for 'dev' and google-genai for 'deploy'
langchain-ollama==0.3.3
langchain-google-genai==2.1


# Others:
# python:
#       => "3.12.0"
#       => "3.12.*" in general
# ollama embeddings 
#       => "mxbai-embeddings-large" (tested and used)
#       => "nomic-embed-text" (if low on GPU memory / CPU inferencing)
# ollama llm models 
#       => "gemma3:latest" or "gemma3:4b" (both are same)(tested, used)
#       => "gemma3:1b" (if you're low on GPU memory / CPU inferencing)
