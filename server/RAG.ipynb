{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f947b1b",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae8306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat:\n",
    "from operator import itemgetter\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# History\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.messages import trim_messages\n",
    "from langchain_core.runnables import RunnableWithMessageHistory, RunnablePassthrough\n",
    "# Load\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# Store\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# Retrieve\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c8fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from llm import get_response_stream, get_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9034491",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in get_response(\"hello\", dummy=True):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e0efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9fd40ed",
   "metadata": {},
   "source": [
    "## LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "# Gemma3 context size -> 128K (1,31,072)\n",
    "# 30k -> 91% RAM, 91% GPU\n",
    "# 25k -> 82% RAM, 89% GPU\n",
    "# 15k -> 66% RAM, 87% GPU\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:latest\", temperature=\"1\",\n",
    "    #  num_predict=MAX_OUTPUT_TOKENS,\n",
    "    num_gpu=35, num_ctx=20000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e663687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown(llm.invoke(\"write a story\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa74252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aef563fc",
   "metadata": {},
   "source": [
    "## Template:\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <strong>Limitations:</strong> Listed are some base assumptions in certain components of langchain components.\n",
    "</div>\n",
    "\n",
    "- `CreateHistoryAwareRetriever` assumes the latest-user-message key to be `input`\n",
    "- `Trimmer` assumes the `ChatHistory` key to be `messages`\n",
    "- `CreateStuffDocumentChain` assumes returns the clubbed `docs` in key `context`\n",
    "- To overcome this, you need to use `RunnablePassthrough` or RunnableMap and assign those keys and variables accordingly.\n",
    "- But remember, you need to manually set such things for all the variables which u are using different than default.\n",
    "\n",
    "- So it's always good to follow the default keys and avoid complexity in chains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9078666d",
   "metadata": {},
   "source": [
    "### Chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template_chat = ChatPromptTemplate.from_messages(\n",
    "#     messages=[\n",
    "#         SystemMessage(\n",
    "#             \"You are a helpful assistant. You answer the question asked based on the chat history and also the Documents attached in context. Answer factually and clearly. State the source in answer wherever possible. Use various markdown features in response. \\n<CONTEXT>\\n{context}\\n</CONTEXT>\"),\n",
    "#         MessagesPlaceholder(variable_name=\"messages\"),\n",
    "#         HumanMessage(\"{input}\")\n",
    "#     ]\n",
    "# )\n",
    "# template_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca015456",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_chat = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\"system\",  \"\".join([\n",
    "            \"You are a highly knowledgeable and helpful AI assistant.\\n\"\n",
    "            \"You are provided with the user's chat history and external documents to assist in your response.\\n\\n\"\n",
    "            \"Your task is to:\\n\"\n",
    "            \"- Accurately and clearly answer the user's latest question.\\n\"\n",
    "            \"- Incorporate any relevant information from the context documents enclosed below.\\n\"\n",
    "            # \"- Reference the source(s) whenever applicable.\\n\"\n",
    "            \"- Use appropriate markdown formatting for clarity and readability (e.g., bullet points, headings, code blocks, tables).\\n\\n\"\n",
    "            \"Contextual Documents:\\n\"\n",
    "            \"<CONTEXT>{context}</CONTEXT>\"\n",
    "        ])),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "template_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc0a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tokens in this System message and pass rest of the max possible chat history:\n",
    "# trim_keep = model_context - template_tokens - 250 (safe side)\n",
    "# template_chat.messages[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05028a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6659d3b3",
   "metadata": {},
   "source": [
    "### Summarize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template_summarize = ChatPromptTemplate.from_messages(\n",
    "#     messages=[\n",
    "#         SystemMessage(\n",
    "#             \"You are a Summarizing expert. You are given with a complete chat history and the latest user message in end of it. The latest message might have some content which refers to some part in history. You have to compile everything and return a single prompt, which will have a standalone question which can be completely understood without any chat history. So, give me a single prompt which will be helpful in retrieving the most relevant docs to latest message.\"),\n",
    "#         MessagesPlaceholder(variable_name=\"messages\"),\n",
    "#         HumanMessage(\"{input}\")\n",
    "#     ]\n",
    "# )\n",
    "# template_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d263f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_summarize = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\"system\", \"\".join([\n",
    "            \"You are an expert at summarizing conversations into standalone prompts.\\n\"\n",
    "            \"You are given a complete chat history, ending with the user's latest message.\\n\\n\"\n",
    "            \"Your task is to:\\n\"\n",
    "            \"- Understand the entire conversation context.\\n\"\n",
    "            \"- Identify references in the latest user message that relate to earlier messages.\\n\"\n",
    "            \"- Create a single clear, concise, and standalone question or prompt.\\n\"\n",
    "            \"- This final prompt should be fully understandable without needing the prior conversation.\\n\"\n",
    "            \"- It will be used to retrieve the most relevant documents.\\n\\n\"\n",
    "            \"Only return the rewritten standalone prompt. Do not add explanations or formatting.\"\n",
    "        ])),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"human\",\n",
    "         \"{input}. \\n\\n **Make one standalone prompt as asked!**\")\n",
    "    ]\n",
    ")\n",
    "template_summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc331c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tokens in this System message and pass rest of the max possible chat history:\n",
    "# trim_keep = model_context - template_tokens - (1000tok/doc * n-docs) - 250 (safe side)\n",
    "# template_summarize.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7fa2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e81f3ce2",
   "metadata": {},
   "source": [
    "## Chat Message History:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bd691",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    # print(\"*\"*40, session_id, \"*\"*40)\n",
    "    if session_id not in chat_histories:\n",
    "        chat_histories[session_id] = ChatMessageHistory()\n",
    "        # log here for creation of new chat history\n",
    "        print(f\"Created chat hist for session id: `{session_id}`\")    \n",
    "    return chat_histories[session_id]\n",
    "\n",
    "get_session_history(\"abv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8348a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_session_history(\"abv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d912cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e8716e2",
   "metadata": {},
   "source": [
    "### Trimmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39350a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "# For summary 15k chat + 1k system and all\n",
    "trim_summary = trim_messages(\n",
    "    max_tokens=15000,\n",
    "    strategy=\"last\", token_counter=llm, start_on=\"human\",\n",
    "    allow_partial=True,  # include_system=True,\n",
    ")\n",
    "\n",
    "# For chat 10k chat + 5*1k docs + 1k system and all\n",
    "trim_chat = trim_messages(\n",
    "    max_tokens=10000,\n",
    "    strategy=\"last\", token_counter=llm, start_on=\"human\",\n",
    "    allow_partial=True,  # include_system=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b73aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34a387c0",
   "metadata": {},
   "source": [
    "## VectorStore:\n",
    "### Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1fa76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large:latest\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e69b0",
   "metadata": {},
   "source": [
    "### Loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec65bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = PyMuPDFLoader(file_path=\"./assets/pdf_w_text.pdf\", extract_tables='markdown', extract_images=True).load()\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f927b8aa",
   "metadata": {},
   "source": [
    "### Splitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5247c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=750, chunk_overlap=150,\n",
    ")\n",
    "splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d840a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afd1b33c",
   "metadata": {},
   "source": [
    "### Database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c108c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted = splitter.split_documents(file)\n",
    "splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This initialization needs 4 param, so rather moving to adding one doc manually.\n",
    "database = FAISS.from_documents(documents=splitted, embedding=embeddings)\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dba060",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr(splitted[0].page_content))\n",
    "print(len(splitted[0].page_content.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b24c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9df58878",
   "metadata": {},
   "source": [
    "### Retriever:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcfabaf",
   "metadata": {},
   "source": [
    "- So for 750 chars, there are appx 95 word (max 150)\n",
    "- In order to retrieve the 3k tokens, we need to have 3k/150 = 20 chunks\n",
    "- So, set k=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01408ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = database.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 20}\n",
    ")\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e90fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"fun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b38c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6323de82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a1aca4c",
   "metadata": {},
   "source": [
    "## Summarizer:\n",
    "\n",
    "- Old method.\n",
    "- This is too much hard-coded, switch to the retrieval method with the create_stuff_chain to ingest the documents and get the answer in one single chain call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b9324",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough().assign(messages=itemgetter(\"messages\") | trim_chat)\n",
    "    | template_summarize | llm | StrOutputParser())\n",
    "\n",
    "summarizer_llm = RunnableWithMessageHistory(\n",
    "    runnable=chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0e8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_histories[10] = ChatMessageHistory()\n",
    "chat_histories[10].messages = [\n",
    "    HumanMessage(\"Hello, I'm Bhushan, What is your name?\"),\n",
    "    AIMessage(\"I am an AI assistant. I am not a human like you.\"),\n",
    "    HumanMessage(\"What is Artificial General Intelligence?\"),\n",
    "    AIMessage(\"Artificial General Intelligence (AGI) refers to highly autonomous systems that outperform humans at most economically valuable work.\"),\n",
    "]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_llm.invoke(\n",
    "    input={\"input\": \"So it's not achieved yet?\", },\n",
    "    config={\"configurable\": {\"session_id\": 10}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_histories[10].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba9fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04c1d7ad",
   "metadata": {},
   "source": [
    "## Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 User Input + Chat History > Summarizer Template > Standalone Que > Get Docs\n",
    "summarize_chain = create_history_aware_retriever(llm, retriever, template_summarize)\n",
    "\n",
    "# 4 Multiple Docs > Combine All > Chat Template > Final Output\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=template_chat)\n",
    "\n",
    "# 2 Input + Chat History > [ `Summarizer Template` > `Get Docs` ] > [ `Combine` > `Chat Template` ] > Output\n",
    "rag_chain = create_retrieval_chain(summarize_chain, qa_chain)\n",
    "\n",
    "# 1 Final main chain:\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    runnable=rag_chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"messages\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n",
    "conversational_rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    input={\"input\":\"Hello, I am Bhushan. What abt u?\"},\n",
    "    config={\"configurable\":{\"session_id\":120}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483077d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3fb841d",
   "metadata": {},
   "source": [
    "## Runnable With History:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough(name=\"Trim Chat History\").assign(messages=itemgetter(\"messages\") | trim_chat)\n",
    "    | template_chat | llm | StrOutputParser())\n",
    "\n",
    "chat_llm = RunnableWithMessageHistory(\n",
    "    runnable=chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff80224",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm.invoke(\n",
    "    input={\n",
    "        \"input\": \"Hello, I'm Bhushan, What is your name?\",\n",
    "        \"context\": \"This is some random document which contains some random information.\"\n",
    "    },\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"session_id\": 15\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d49b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm.invoke(\n",
    "    input={\n",
    "        \"input\": \"What did we discuss?\",\n",
    "        \"context\": \"There is no context available for this question.\"\n",
    "    },\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"session_id\": 15\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572626cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a284c06c",
   "metadata": {},
   "source": [
    "- If () add option to paste link and scrap whole content from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response_stream(prompt: str) -> Generator[str, None, None]:\n",
    "    \"\"\"Returns the response from LLM for given prompt using Generator.\"\"\"\n",
    "    # Chat Prompt Template:\n",
    "    template = ChatPromptTemplate.from_messages(\n",
    "        messages=[\n",
    "            (\"system\", \"You are a helpful assistant '{llm_name}' who responds to questions in not more than 20 sentences. You can use markdown and code blocks to format your answers. You can also use emojis to make your answers more engaging. Please be concise and clear in your responses.\"),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"{new_input}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Ensure model and provider are selected:\n",
    "    if not st.session_state.provider:\n",
    "        raise ValueError(\n",
    "            \"Provider not selected, please select a provider first.\")\n",
    "\n",
    "    if not st.session_state.model:\n",
    "        raise ValueError(\"Model not selected, please select a model first.\")\n",
    "\n",
    "    # Set-up LLM:\n",
    "    llm = None\n",
    "\n",
    "    if st.session_state.provider == \"OpenAI\":\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        llm = ChatOpenAI(\n",
    "            model=st.session_state.model, api_key=st.secrets.OpenAI.API_KEY)\n",
    "\n",
    "    elif st.session_state.provider == \"Groq\":\n",
    "        from langchain_groq import ChatGroq\n",
    "        llm = ChatGroq(\n",
    "            model=st.session_state.model, api_key=st.secrets.Groq.API_KEY)\n",
    "\n",
    "    elif st.session_state.provider == \"Ollama\":\n",
    "        from langchain_ollama import ChatOllama\n",
    "        llm = ChatOllama(model=st.session_state.model)\n",
    "\n",
    "    else:\n",
    "        st.error(\"Some un-expected error occurred...\", icon=\"ðŸ¤–\")\n",
    "\n",
    "    # Output parser:\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    # Chain with trimmer:\n",
    "    # Trimmer:\n",
    "    trimmer = trim_messages(\n",
    "        max_tokens=2000, strategy=\"last\",\n",
    "        token_counter=llm, include_system=False,\n",
    "        allow_partial=True, start_on=HumanMessage\n",
    "    )\n",
    "\n",
    "    # Chain them all:\n",
    "    chain = (\n",
    "        # Set \"messages\" key equal to chat_history\n",
    "        RunnablePassthrough.assign(\n",
    "            messages=itemgetter(\"chat_history\") | trimmer)\n",
    "        # Set \"chat_history\" key equal to \"messages\" (default output key of trimmer)\n",
    "        | RunnablePassthrough.assign(chat_history=itemgetter(\"messages\"))\n",
    "        | template\n",
    "        | llm\n",
    "        | parser\n",
    "    )\n",
    "    # Tested and WORKING ðŸ¥³\n",
    "\n",
    "    # # Chain without the trimmer:\n",
    "    # # Comment out the trimmer and chain above to use this:\n",
    "    # chain = (\n",
    "    #     template\n",
    "    #     | llm\n",
    "    #     | parser\n",
    "    # )\n",
    "\n",
    "    llm_with_history = RunnableWithMessageHistory(\n",
    "        runnable=chain,\n",
    "        get_session_history=get_session_history,\n",
    "        input_messages_key=\"new_input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "    )\n",
    "\n",
    "    # Run the chain (streaming):\n",
    "    yield from llm_with_history.stream(\n",
    "        input={\n",
    "            \"new_input\": prompt,\n",
    "            \"llm_name\": st.session_state.name,\n",
    "        }\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
